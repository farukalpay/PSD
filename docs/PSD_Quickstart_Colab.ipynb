{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac26589",
   "metadata": {},
   "source": [
    "\n",
    "# 10\u2011Minute Quickstart Challenge: Escaping Saddle Points with a Minimal PSD (Perturbed Saddle-escape Descent)\n",
    "\n",
    "**Goal:** Build strong intuition about **saddle points** and a hands\u2011on **minimal PSD** (PSD\u2011Lite) that escapes them on a small 2D example.  \n",
    "Complete the following challenges:\n",
    "1. Visualize a classic saddle surface and analyze why gradient descent may stall.\n",
    "2. Implement a minimal PSD-Lite with finite-difference curvature probes (no Hessians).\n",
    "3. Observe PSD-Lite perturb and escape when the gradient is small.\n",
    "4. (Appendix) Integrate the curvature probe into a deep-learning training loop.\n",
    "\n",
    "> This is an **educational quickstart**: simple, readable code that matches the *spirit* of PSD. It uses finite differences and conservative defaults, not exact constants from any paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2be082",
   "metadata": {},
   "source": [
    "\n",
    "## Challenge 0: Setup\n",
    "\n",
    "This notebook uses only standard Python + NumPy + Matplotlib (PyTorch is only needed for the appendix template).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b89b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on Google Colab, you can optionally ensure dependencies:\n",
    "# !pip install --quiet numpy matplotlib torch torchvision\n",
    "import sys, math, time, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Global plot defaults for readability (no seaborn, no custom colors)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c0d84",
   "metadata": {},
   "source": [
    "\n",
    "## Challenge 1: Visual intuition\u2014what is a saddle point?\n",
    "\n",
    "We'll start with a classic saddle function:  \n",
    "\\[ f(x, y) = x^2 - y^2. \\]\n",
    "\n",
    "- At the origin (0,0): gradient is zero \u2192 a stationary point.  \n",
    "- Curvature is **up** along x and **down** along y \u2192 **saddle**.\n",
    "- Pure **gradient descent** gets **stuck** if started exactly at (0,0) (since the gradient is exactly zero there).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the saddle surface and a contour view\n",
    "xs = np.linspace(-2, 2, 200)\n",
    "ys = np.linspace(-2, 2, 200)\n",
    "X, Y = np.meshgrid(xs, ys)\n",
    "Z = X**2 - Y**2\n",
    "\n",
    "# 3D surface\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z, alpha=0.85, edgecolor='none')\n",
    "ax.set_title(\"Saddle: f(x,y) = x^2 - y^2\")\n",
    "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_zlabel(\"f(x,y)\")\n",
    "plt.show()\n",
    "\n",
    "# 2D contour\n",
    "plt.figure(figsize=(6,5))\n",
    "cs = plt.contour(X, Y, Z, levels=20)\n",
    "plt.clabel(cs, inline=True, fontsize=8)\n",
    "plt.title(\"Contours of f(x,y) = x^2 - y^2 (Saddle)\")\n",
    "plt.xlabel(\"x\"); plt.ylabel(\"y\")\n",
    "plt.gca().set_aspect('equal', 'box')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efbe42",
   "metadata": {},
   "source": [
    "\n",
    "## Challenge 2: Minimal PSD-Lite (finite-difference probe; no Hessians)\n",
    "\n",
    "**Key idea:** When the gradient becomes **small**, we **probe curvature** along random directions using a simple **central difference** of the function values:\n",
    "\\[ q(v) = \\frac{f(x + h v) - 2 f(x) + f(x - h v)}{h^2}. \\]\n",
    "\n",
    "- If any probe yields **negative** curvature below a threshold (\\( q(v) < -\\gamma \\)), we **perturb** along that direction and take a short sequence of **gradient steps** (an **escape episode**).\n",
    "- Otherwise, we declare an approximate second\u2011order stationary point (no strong negative curvature found).\n",
    "\n",
    "We'll use a slightly non\u2011quadratic function with a saddle at the origin, so curvature changes with position (more realistic than a pure quadratic):\n",
    "\\[ f(x,y) = \\tfrac{1}{4}x^4 - \\tfrac{1}{4}y^4 - \\tfrac{1}{2}(x^2 - y^2) + 0.1\\,x\\,y. \\]\n",
    "\n",
    "> **Why this function?** It has a saddle at (0,0), non\u2011constant curvature (so finite\u2011difference probing is meaningful), and simple derivatives for gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd119347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the example function and its gradient\n",
    "def f_xy(xy):\n",
    "    x, y = xy\n",
    "    return 0.25*x**4 - 0.25*y**4 - 0.5*(x**2 - y**2) + 0.1*x*y\n",
    "\n",
    "def grad_f_xy(xy):\n",
    "    x, y = xy\n",
    "    dfx = x**3 - x + 0.1*y\n",
    "    dfy = -y**3 + y + 0.1*x\n",
    "    return np.array([dfx, dfy], dtype=float)\n",
    "\n",
    "def central_diff_curvature(xy, v, h):\n",
    "    # q(v) = [f(x + h v) - 2 f(x) + f(x - h v)] / h^2\n",
    "    v = v / np.linalg.norm(v)\n",
    "    return (f_xy(xy + h*v) - 2.0*f_xy(xy) + f_xy(xy - h*v)) / (h**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741f88d",
   "metadata": {},
   "source": [
    "\n",
    "### Challenge: PSD-Lite algorithm (educational defaults)\n",
    "\n",
    "We implement a minimal version with easy parameters:\n",
    "- Gradient threshold: `eps_g` (enter escape logic when \\(\\|\\nabla f\\| \\le eps_g\\)).  \n",
    "- Probe radius: `h` (finite\u2011difference step).  \n",
    "- Negative\u2011curvature threshold: `gamma`.  \n",
    "- Perturbation radius: `r`.  \n",
    "- Escape episode length: `T` gradient steps with step size `eta`.\n",
    "\n",
    "> These are **teaching defaults**. For production or theoretical guarantees, you'd calibrate parameters more carefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2581b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def psd_lite(\n",
    "    x0=np.array([0.0, 0.0]),\n",
    "    eps_g=1e-3,\n",
    "    h=1e-2,\n",
    "    gamma=1e-2,\n",
    "    r=5e-2,\n",
    "    T=30,\n",
    "    eta=0.1,\n",
    "    max_iters=2000,\n",
    "    m_probes=16,\n",
    "    seed=0,\n",
    "    verbose=True\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = x0.astype(float).copy()\n",
    "    traj = [x.copy()]\n",
    "    episodes = 0\n",
    "    it = 0\n",
    "    logs = []\n",
    "\n",
    "    while it < max_iters:\n",
    "        g = grad_f_xy(x)\n",
    "        gnorm = np.linalg.norm(g)\n",
    "\n",
    "        if gnorm > eps_g:\n",
    "            # plain gradient step\n",
    "            x = x - eta * g\n",
    "            traj.append(x.copy())\n",
    "            it += 1\n",
    "            continue\n",
    "\n",
    "        # gradient is small: probe for negative curvature\n",
    "        found_nc = False\n",
    "        best_q = float('inf')\n",
    "        best_v = None\n",
    "        for _ in range(m_probes):\n",
    "            v = rng.normal(size=2)\n",
    "            q = central_diff_curvature(x, v, h)\n",
    "            if q < best_q:\n",
    "                best_q, best_v = q, v\n",
    "\n",
    "        if verbose:\n",
    "            logs.append(dict(iter=it, gnorm=gnorm, best_q=best_q))\n",
    "\n",
    "        if best_q < -gamma:\n",
    "            # Negative curvature detected: do an escape episode\n",
    "            episodes += 1\n",
    "            # Random perturbation (small push)\n",
    "            xi = rng.normal(size=2)\n",
    "            xi = r * xi / np.linalg.norm(xi)\n",
    "            y = x + xi\n",
    "\n",
    "            for _ in range(T):\n",
    "                y = y - eta * grad_f_xy(y)\n",
    "                traj.append(y.copy())\n",
    "\n",
    "            x = y\n",
    "            it += T\n",
    "        else:\n",
    "            # Approximate SOSP\n",
    "            break\n",
    "\n",
    "    return np.array(traj), episodes, logs, x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed404f27",
   "metadata": {},
   "source": [
    "\n",
    "### Challenge 3: Run PSD-Lite vs. plain Gradient Descent (starting at the saddle)\n",
    "\n",
    "- **GD** starting at (0,0) does **nothing** (gradient is exactly zero).  \n",
    "- **PSD\u2011Lite** perturbs and runs a short **escape episode**, then continues with gradient steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fccf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_gd(x0, eta=0.1, steps=60):\n",
    "    x = x0.astype(float).copy()\n",
    "    traj = [x.copy()]\n",
    "    for _ in range(steps):\n",
    "        x = x - eta * grad_f_xy(x)\n",
    "        traj.append(x.copy())\n",
    "    return np.array(traj)\n",
    "\n",
    "x0 = np.array([0.0, 0.0])  # exactly at the saddle\n",
    "\n",
    "# Plain GD\n",
    "gd_traj = run_gd(x0, eta=0.1, steps=60)\n",
    "\n",
    "# PSD-Lite\n",
    "traj, episodes, logs, x_final = psd_lite(\n",
    "    x0=x0, eps_g=1e-6, h=1e-2, gamma=1e-2, r=5e-2, T=30, eta=0.1,\n",
    "    max_iters=2000, m_probes=16, seed=42, verbose=True\n",
    ")\n",
    "\n",
    "print(f\"Episodes triggered: {episodes}\")\n",
    "print(f\"Final point (PSD-Lite): {x_final}, f = {f_xy(x_final):.6f}\")\n",
    "\n",
    "# Plot contours + trajectories\n",
    "xs = np.linspace(-1.5, 1.5, 300)\n",
    "ys = np.linspace(-1.5, 1.5, 300)\n",
    "X, Y = np.meshgrid(xs, ys)\n",
    "Z = 0.25*X**4 - 0.25*Y**4 - 0.5*(X**2 - Y**2) + 0.1*X*Y\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "cs = plt.contour(X, Y, Z, levels=30)\n",
    "plt.clabel(cs, inline=True, fontsize=8)\n",
    "plt.plot(gd_traj[:,0], gd_traj[:,1], marker='.', linewidth=1, label='GD from (0,0)')\n",
    "plt.plot(traj[:,0], traj[:,1], marker='.', linewidth=1, label='PSD-Lite from (0,0)')\n",
    "plt.scatter([0],[0], s=60, marker='x', label='Start (saddle)')\n",
    "plt.title('GD vs PSD-Lite (contours)')\n",
    "plt.xlabel('x'); plt.ylabel('y')\n",
    "plt.gca().set_aspect('equal', 'box')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14c40b8",
   "metadata": {},
   "source": [
    "\n",
    "### Challenge 4: Inspect escape logs\n",
    "Look at when the gradient got tiny and the probe found negative curvature (most negative `q`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59930f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show a few log lines\n",
    "for i, rec in enumerate(logs[:10]):\n",
    "    print(f\"iter={rec['iter']:>4}  ||grad||={rec['gnorm']:.2e}  best_q={rec['best_q']:.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a506602",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Optional Challenge: Integrate a curvature probe into a DL training loop (template)\n",
    "\n",
    "Below is a **template** for adding a finite\u2011difference curvature probe to a PyTorch training loop.\n",
    "It samples random parameter directions, uses a small central\u2011difference step `h` on the **loss**,\n",
    "and if it detects strong negative curvature (`q < -gamma`), it applies a small **parameter perturbation**.\n",
    "\n",
    "> This is deliberately **minimal** and intended as a learning scaffold \u2014 not a drop\u2011in optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2b2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Optional) PyTorch template for adding a curvature probe to training\n",
    "# Note: This is a template; it will run, but it's configured for tiny demos / unit tests.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def flatten_params(model):\n",
    "    return torch.cat([p.detach().flatten() for p in model.parameters()])\n",
    "\n",
    "def assign_flat_params_(model, flat, like_params):\n",
    "    # Write a flat vector into model parameters (in-place)\n",
    "    idx = 0\n",
    "    with torch.no_grad():\n",
    "        for p in like_params:\n",
    "            num = p.numel()\n",
    "            p.copy_(flat[idx:idx+num].view_as(p))\n",
    "            idx += num\n",
    "\n",
    "def loss_closure(model, x, y):\n",
    "    # Simple supervised loss\n",
    "    logits = model(x)\n",
    "    return F.cross_entropy(logits, y)\n",
    "\n",
    "@torch.no_grad()\n",
    "def probe_negative_curvature(model, data_batch, h=1e-3, m_dirs=4, gamma=1e-3, device='cpu'):\n",
    "    # Central-difference probe on the loss along random parameter directions\n",
    "    model.eval()\n",
    "    (x, y) = data_batch\n",
    "    x = x.to(device); y = y.to(device)\n",
    "\n",
    "    # Flatten parameters\n",
    "    like_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    base = flatten_params(model).to(device)\n",
    "\n",
    "    # Evaluate base loss\n",
    "    base_loss = loss_closure(model, x, y).item()\n",
    "\n",
    "    best_q = float('inf')\n",
    "    best_dir = None\n",
    "\n",
    "    for _ in range(m_dirs):\n",
    "        # Random direction with unit norm\n",
    "        d = torch.randn_like(base)\n",
    "        d = d / (d.norm() + 1e-12)\n",
    "\n",
    "        # Central difference on loss\n",
    "        assign_flat_params_(model, base + h*d, like_params)\n",
    "        lp = loss_closure(model, x, y).item()\n",
    "        assign_flat_params_(model, base - h*d, like_params)\n",
    "        lm = loss_closure(model, x, y).item()\n",
    "        assign_flat_params_(model, base, like_params)\n",
    "\n",
    "        q = (lp - 2*base_loss + lm) / (h*h)\n",
    "        if q < best_q:\n",
    "            best_q, best_dir = q, d\n",
    "\n",
    "    return best_q, best_dir  # if best_q < -gamma, consider perturbation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd052367",
   "metadata": {},
   "source": [
    "\n",
    "### Optional Challenge: Minimal training loop sketch with probe (for small toy models)\n",
    "\n",
    "- Every few steps, if the gradient norm is small, we call `probe_negative_curvature`.\n",
    "- If `q < -gamma`, we **perturb parameters** a little along `best_dir`, then continue training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ab726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sketch of usage (CPU; tiny batch; for illustration only).\n",
    "# You can adapt this into your own training script.\n",
    "\n",
    "class TinyMLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, hid=16, out_dim=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hid)\n",
    "        self.fc2 = nn.Linear(hid, out_dim)\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "device = 'cpu'\n",
    "model = TinyMLP().to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.0)\n",
    "\n",
    "# Fake data (replace with real dataloader)\n",
    "x = torch.randn(64, 2).to(device)\n",
    "y = (x[:,0] > x[:,1]).long().to(device)  # toy labels\n",
    "batch = (x, y)\n",
    "\n",
    "eps_g = 1e-4\n",
    "gamma = 1e-3\n",
    "h = 1e-3\n",
    "r = 1e-2   # perturbation magnitude in parameter space\n",
    "check_every = 10\n",
    "\n",
    "for step in range(60):\n",
    "    opt.zero_grad()\n",
    "    loss = loss_closure(model, x, y)\n",
    "    loss.backward()\n",
    "    gnorm = torch.sqrt(sum((p.grad**2).sum() for p in model.parameters())).item()\n",
    "    opt.step()\n",
    "\n",
    "    if step % check_every == 0 and gnorm < eps_g:\n",
    "        q, d = probe_negative_curvature(model, batch, h=h, m_dirs=4, gamma=gamma, device=device)\n",
    "        if q < -gamma and d is not None:\n",
    "            # Apply a small parameter perturbation along d\n",
    "            base = flatten_params(model)\n",
    "            base = base + r * d\n",
    "            assign_flat_params_(model, base, [p for p in model.parameters() if p.requires_grad])\n",
    "            print(f\"[step {step}] Escape episode triggered: q={q:.3e}, gnorm={gnorm:.2e}\")\n",
    "        else:\n",
    "            print(f\"[step {step}] Likely near SOSP: q={q:.3e}, gnorm={gnorm:.2e}\")\n",
    "\n",
    "print(\"Done. (This was a tiny sketch; adapt to your real model and data.)\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}